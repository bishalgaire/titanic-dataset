{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic-dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bishalgaire/titanic-dataset/blob/master/titanic_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CVFrUdM0ObjX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import pandas as pd\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgbiY8gFPEsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "listed = drive.ListFile({'q': \"title contains 'titanic3.xls' and '1apc5qMMRHGgWvG7V2l5Y9xNeUBKO30Wr' in parents\"}).GetList()\n",
        "download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(download_path)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "output_file = os.path.join(download_path, 'titanic3.xls')\n",
        "temp_file = drive.CreateFile({'id':'1JYOujokPMPcCpoYvmL8CWUDzj57Gvezl'})\n",
        "temp_file.GetContentFile(output_file)\n",
        "data = pd.read_excel(output_file,sep='delimiter',encoding = \"ISO-8859-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WHxm3J3ZPYdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8ee08113-88af-4f49-bb27-b24210490961"
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pclass</th>\n",
              "      <th>survived</th>\n",
              "      <th>name</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>ticket</th>\n",
              "      <th>fare</th>\n",
              "      <th>cabin</th>\n",
              "      <th>embarked</th>\n",
              "      <th>boat</th>\n",
              "      <th>body</th>\n",
              "      <th>home.dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allen, Miss. Elisabeth Walton</td>\n",
              "      <td>female</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24160</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>B5</td>\n",
              "      <td>S</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>St Louis, MO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allison, Master. Hudson Trevor</td>\n",
              "      <td>male</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Miss. Helen Loraine</td>\n",
              "      <td>female</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>135.0</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pclass  survived                                             name     sex  \\\n",
              "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
              "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
              "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
              "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
              "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
              "\n",
              "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
              "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
              "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
              "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
              "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
              "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
              "\n",
              "                         home.dest  \n",
              "0                     St Louis, MO  \n",
              "1  Montreal, PQ / Chesterville, ON  \n",
              "2  Montreal, PQ / Chesterville, ON  \n",
              "3  Montreal, PQ / Chesterville, ON  \n",
              "4  Montreal, PQ / Chesterville, ON  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "P6feUHcqPa2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11AGkLQsPrKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3527f7a3-befe-4b15-eda5-2545149fa6ae"
      },
      "cell_type": "code",
      "source": [
        "data.columns.values"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch',\n",
              "       'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "n2fi8JjxPvkv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.drop([\"ticket\", \"home.dest\",\"fare\",\"cabin\",\"boat\",\"body\",\"name\"], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YneKFv3P4P4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mean = data[\"age\"].mean()\n",
        "std = data[\"age\"].std()\n",
        "\n",
        "data[\"age\"].fillna(mean, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fGn2kINPP9fD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data[\"embarked\"].fillna(\"S\", inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbMpjgF0QAM8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
        "data['embarked'] = data['embarked'].map(ports)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWzpAFxSQFh4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data['sex'] = data.sex.apply(lambda sex: 1 if sex == 'male' else 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68mEpbNIQI16",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X=data.iloc[:,[0,2,3,4,5,6]].values#create a matrix X and Y\n",
        "Y=data.iloc[:,1].values\n",
        "X = X.reshape(X.shape[0], X.shape[1])  # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "Y = Y.reshape(Y.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fq6iawBsRbWm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install -U scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PFLrSxVRj5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X=StandardScaler()\n",
        "X_train=sc_X.fit_transform(X_train)\n",
        "X_test=sc_X.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dox8aiFNR57B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "437ef147-cc03-400c-d0eb-f3c406c0671f"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "reg=LogisticRegression(random_state=0)\n",
        "reg.fit(X_train,Y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "NVjn1x8ER8sO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "60504b0a-4468-4495-cc88-5896deddda4f"
      },
      "cell_type": "code",
      "source": [
        "Y_expected=reg.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(Y_test,Y_expected)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[178,  30],\n",
              "       [ 40,  80]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "KJPx4tNfSCbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0622d29c-2908-4edf-d2f1-6563f3164fc1"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn= KNeighborsClassifier(n_neighbors=6)\n",
        "knn.fit(X_train,Y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
              "           weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "c2rJuncFSL9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4cc6bfa-ad4f-423a-a31c-f1658ec5cf33"
      },
      "cell_type": "code",
      "source": [
        "Y_expected=knn.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(Y_test,Y_expected)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[186,  22],\n",
              "       [ 49,  71]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "jwTvUicfSagr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "2da0c313-e3a5-4b07-99c1-240eeff2b897"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfg=RandomForestClassifier(n_estimators=100,criterion ='entropy',random_state=0)\n",
        "rfg.fit(X_train,Y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
              "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
              "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "Q--2CB2mTITx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3515b264-bae2-4d93-ee8f-b63db0fc179b"
      },
      "cell_type": "code",
      "source": [
        "Y_expected=rfg.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(Y_test,Y_expected)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[183,  25],\n",
              "       [ 45,  75]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "BzDunMI7TNtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "50ecf396-4382-4a8f-e63f-a80fe306b99f"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc=DecisionTreeClassifier(criterion='entropy',min_samples_split=5)\n",
        "dtc.fit(X_train,Y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=5,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "e2pJKA1vTfkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f94f5c1f-e8f8-469a-8e2e-c4bf6d0b33df"
      },
      "cell_type": "code",
      "source": [
        "Y_expected=dtc.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(Y_test,Y_expected)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[186,  22],\n",
              "       [ 49,  71]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "Po_lhRkATtIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "52905b50-5bc9-4609-c48e-d08979ca6fe3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train,Y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "bKtB_Iurb643",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b6c2bcfd-b429-4a3a-b055-d331e1440d00"
      },
      "cell_type": "code",
      "source": [
        "Y_expected=gnb.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(Y_test,Y_expected)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[169,  39],\n",
              "       [ 35,  85]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "W-sw53Lzb_Ea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train=X_train.T\n",
        "Y_train=Y_train.T\n",
        "X_test=X_test.T\n",
        "Y_test=Y_test.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVa1hYgwf1Of",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "  \n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "  \n",
        "  \n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OqxcgQSDgC_K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    Z =np.dot(W,A)+b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache\n",
        "  \n",
        "  \n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "  \n",
        "  \n",
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hd2G5_fggX47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "    #m=981\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = -(1/m)*np.sum((Y.T*np.log(AL.T))+((1-Y.T)*np.log(1-AL.T)))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qLKvdv6TgzRA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k4LLJpi7g511",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "  \n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "  \n",
        "  \n",
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "   \n",
        "\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IMcQ9OEWhQto",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hU8asarQhSky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization.\n",
        "    ### START CODE HERE ###\n",
        "    parameters =initialize_parameters_deep(layers_dims)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "        # Backward propagation.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        grads =L_model_backward(AL, Y, caches)\n",
        "        ### END CODE HERE ###\n",
        " \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        ### END CODE HERE ###\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SVpqOVT9hnzd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    #print (\"predictions: \" + str(p))\n",
        "    #print (\"true labels: \" + str(y))\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "27Ap-o6rhXMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "0b0e56b2-6443-49a4-8d8f-c49425f653dc"
      },
      "cell_type": "code",
      "source": [
        "layers_dims = [6, 20, 7, 5, 1]\n",
        "parameters = L_layer_model(X_train, Y_train, layers_dims, num_iterations = 2500, print_cost = True)\n",
        "predict(X_test, Y_test, parameters)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.711805\n",
            "Cost after iteration 100: 0.691562\n",
            "Cost after iteration 200: 0.677561\n",
            "Cost after iteration 300: 0.665989\n",
            "Cost after iteration 400: 0.653756\n",
            "Cost after iteration 500: 0.641063\n",
            "Cost after iteration 600: 0.625798\n",
            "Cost after iteration 700: 0.609158\n",
            "Cost after iteration 800: 0.591073\n",
            "Cost after iteration 900: 0.571468\n",
            "Cost after iteration 1000: 0.551426\n",
            "Cost after iteration 1100: 0.532442\n",
            "Cost after iteration 1200: 0.515498\n",
            "Cost after iteration 1300: 0.501343\n",
            "Cost after iteration 1400: 0.490122\n",
            "Cost after iteration 1500: 0.481585\n",
            "Cost after iteration 1600: 0.475025\n",
            "Cost after iteration 1700: 0.469871\n",
            "Cost after iteration 1800: 0.465562\n",
            "Cost after iteration 1900: 0.462014\n",
            "Cost after iteration 2000: 0.459035\n",
            "Cost after iteration 2100: 0.456528\n",
            "Cost after iteration 2200: 0.454428\n",
            "Cost after iteration 2300: 0.452638\n",
            "Cost after iteration 2400: 0.451086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAFnCAYAAAC7EwBkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8048780487804877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B33u6ug0iDDt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}